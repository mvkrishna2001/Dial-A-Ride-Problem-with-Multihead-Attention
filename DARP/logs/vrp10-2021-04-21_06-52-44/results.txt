actor_net_lr: 0.0001
agent_type: attention
batch_size: 128
beam_width: 10
capacity: 20
critic_net_lr: 0.0001
data_dir: data
decode_len: 16
demand_max: 9
disable_tqdm: True
dropout: 0.1
embedding_dim: 128
entropy_coeff: 0.0
forget_bias: 1.0
gpu: 3
hidden_dim: 128
infer_type: batch
input_dim: 3
is_train: True
load_path: 
log_dir: logs/vrp10-2021-04-21_06-52-44
log_interval: 200
mask_glimpses: True
mask_pointer: True
max_grad_norm: 2.0
model_dir: logs/vrp10-2021-04-21_06-52-44/model
n_cust: 10
n_glimpses: 0
n_nodes: 11
n_process_blocks: 3
n_train: 260000
random_seed: 24601
rnn_layers: 1
save_interval: 10000
stdout_print: True
tanh_exploration: 10.0
task: vrp10
task_name: vrp
test_interval: 200
test_size: 1000
use_tanh: False
# Set random seed to 24601
It took 22.956403493881226s to build the agent.
Training started ...
Train Step: 0 -- Time: 00:00:08 -- Train reward: 7.667901515960693 -- Value: 0.14138606190681458
    actor loss: -157.21856689453125 -- critic loss: 59.23694610595703
Average of greedy in batch-mode: 8.319009780883789 -- std 1.9019404649734497 -- time 2.064592123031616 s
